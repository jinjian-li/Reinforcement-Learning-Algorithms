{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Local Setup\n",
    "\n",
    "If you prefer to work locally, see the following instructions for setting up Python in a virtual environment.\n",
    "You can then ignore the instructions in \"Colab Setup\".\n",
    "\n",
    "If you haven't yet, create a [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) environment using:\n",
    "```\n",
    "conda create --name rl_exercises\n",
    "conda activate rl_exercises\n",
    "```\n",
    "\n",
    "The dependencies can be installed with pip:\n",
    "```\n",
    "pip install matplotlib numpy tqdm scipy\n",
    "```\n",
    "\n",
    "Even if you are running the Jupyter notebook locally, please run the code cells in **Colab Setup**, because they define some global variables required later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Colab Setup\n",
    "\n",
    "Google Colab provides you with a temporary environment for python programming.\n",
    "While this conveniently works on any platform and internally handles dependency issues and such, it also requires you to set up the environment from scratch every time.\n",
    "The \"Colab Setup\" section below will be part of **every** exercise and contains utility that is needed before getting started.\n",
    "\n",
    "There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window).\n",
    "Any changes you make to the Jupyter notebook itself should be saved to your Google Drive.\n",
    "We also save all recordings and logs in it by default so that you won't lose your work in the event of an instance timeout.\n",
    "However, you will need to re-mount your Google Drive and re-install packages with every new instance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:11.534060Z",
     "start_time": "2025-01-23T13:59:11.530810Z"
    }
   },
   "source": [
    "\"\"\"Your work will be stored in a folder called `rl_ws24` by default to prevent Colab \n",
    "instance timeouts from deleting your edits.\n",
    "We do this by mounting your google drive on the virtual machine created in this colab \n",
    "session. For this, you will likely need to sign in to your Google account and allow\n",
    "access to your Google Drive files.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    COLAB = True\n",
    "except ImportError:\n",
    "    COLAB = False\n",
    "\n",
    "# Create paths in your google drive\n",
    "if COLAB:\n",
    "    DATA_ROOT = Path(\"/content/gdrive/My Drive/rl_ws24\")\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    DATA_ROOT_STR = str(DATA_ROOT)\n",
    "    %cd \"$DATA_ROOT\"\n",
    "else:\n",
    "    DATA_ROOT = Path.cwd() / \"rl_ws24\"\n",
    "\n",
    "# Install python packages\n",
    "if COLAB:\n",
    "    %pip install matplotlib numpy tqdm scipy"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We start by importing all the necessary python modules and defining some helper\n",
    "functions which you do not need to change. Still, make sure you are aware of\n",
    "what they do."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.153123Z",
     "start_time": "2025-01-23T13:59:11.544548Z"
    }
   },
   "source": [
    "import time\n",
    "import abc\n",
    "import os\n",
    "from typing import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib.patches import Rectangle\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Set random seed and output paths\n",
    "SEED = 314159\n",
    "OUTPUT_FOLDER = DATA_ROOT / \"exercise_4\" / time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# this function will automatically save your figure into your google drive folder (if correctly mounted!)\n",
    "def save_figure(save_name: str) -> None:\n",
    "    assert save_name is not None, \"Need to provide a filename to save to\"\n",
    "    plt.savefig(os.path.join(OUTPUT_FOLDER, save_name + \".png\"))\n",
    "\n",
    "\n",
    "def plot_metrics(metrics: Dict[str, List[float]]):\n",
    "    \"\"\"\n",
    "    Plots various metrics recorded during training\n",
    "    :param metrics: The metrics to plot\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(metrics) > 0:\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(16, 9))\n",
    "        for position, (key, value) in enumerate(metrics.items()):\n",
    "            plt.subplot(len(metrics), 1, position + 1)\n",
    "            plt.plot(range(len(value)), np.array(value))\n",
    "            plt.ylabel(key.title())\n",
    "            if key == \"mean_reward\":\n",
    "                plt.yscale(\"symlog\")\n",
    "        plt.xlabel(\"Recorded Steps\")\n",
    "        plt.tight_layout()\n",
    "        save_figure(f\"training_metrics\")\n",
    "        plt.clf()\n",
    "        plt.close()\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 4  **Stochastic Search (15 Pts)**\n",
    "\n",
    "This exercise is about Stochastic Search methods for blackbox function optimization. In contrast to many Deep Reinforcement Learning algorithms, Stochastic Search Methods do not rely on any assumptions like the Markov property. As such, they are a highly flexible class of algorithms that can be very powerful in different scenarios. In this homework, we will implement a Canonical Evolutionary Strategy (CES) and the Cross Entropy Method (CEM) as two examples of Stochastic Search Methods. Further, we will look at the math behind MOdel-based Relative Entropy Stochastic Search (MORE), showing how Lagrangian optimization can be used to efficiently optimize a Gaussian search distribution with full covariance.\n",
    "\n",
    "All methods discussed here rely on a Gaussian search distribution. On a high level, they all iteratively execute the following steps to maximize the reward under this distribution.\n",
    "1. draw samples from the search distribution\n",
    "2. evaluate the samples on the target function\n",
    "3. (sort the samples according to the target function, where the best comes first)\n",
    "4. update the parameters of the search distribution with the evaluations\n",
    "\n",
    "Let's start by setting up the Gaussian Distribution class that we will use for all algorithms and the task that we will optimize for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gaussian\n",
    "\n",
    "The next code cell defines a Gaussian class containing the utility functions that are needed for the considered algorithms."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.178560Z",
     "start_time": "2025-01-23T13:59:12.174079Z"
    }
   },
   "source": [
    "class Gaussian:\n",
    "    \"\"\"\n",
    "    A multivariate Gaussian with a full covariance matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean: np.array, covariance: np.array):\n",
    "        if len(mean.shape) < 2:\n",
    "            mean = np.atleast_2d(mean).reshape([-1, 1])\n",
    "        self.task_dimension = mean.shape[0]\n",
    "        self.mean = mean\n",
    "        self.covariance = covariance\n",
    "\n",
    "        self.log_det = None  # log determinant\n",
    "        self.chol_cov = None  # cholesky of the covariant\n",
    "\n",
    "        self.update_params(mean, covariance)\n",
    "\n",
    "        # precompute constant value\n",
    "        self._log_2_pi_k = self.task_dimension * (np.log(2 * np.pi))\n",
    "\n",
    "    def update_params(self, mean: np.array, covariance: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the Gaussian\n",
    "        :param mean: The new mean. Shape: [task_dimension, 1]\n",
    "        :param covariance: The new covariance. Shape: [task_dimension, task_dimension]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(mean.shape) < 2:\n",
    "            mean = np.atleast_2d(mean).reshape([-1, 1])\n",
    "        self.mean = mean\n",
    "        self.covariance = covariance\n",
    "\n",
    "        self.chol_cov = np.linalg.cholesky(self.covariance)\n",
    "        self.log_det = 2 * np.sum(np.log(np.diag(self.chol_cov)))\n",
    "\n",
    "    def sample(self, n_samples: int) -> np.array:\n",
    "        \"\"\"\n",
    "        Draw n_samples samples from the Gaussian\n",
    "        :param n_samples: The number of samples to draw\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z = np.random.normal(size=(n_samples, self.task_dimension)).T\n",
    "        x = self.mean + self.chol_cov @ z\n",
    "        return x.T\n",
    "\n",
    "    @property\n",
    "    def entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute (scalar) entropy of the multivariate Gaussian in closed form\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 0.5 * (self.task_dimension + self._log_2_pi_k + self.log_det)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Point Reacher\n",
    "Our task for this exercise is a planar point reaching task. A robot arm starting at position $(0,0)^T$ with $n$ links of unit length is tasked to reach a point at positions $(0.7\\cdot n, 0)^T$.\n",
    "The action space consist of a continuous angle for each of the $n$ joints.\n",
    "To promote smooth solutions, there is an additional penalty term on the squared angles.\n",
    "\n",
    "You do **not** need to adapt the code for this task."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.186140Z",
     "start_time": "2025-01-23T13:59:12.179744Z"
    }
   },
   "source": [
    "class PointReacher:\n",
    "\n",
    "    def __init__(self, num_links: int, likelihood_std: float, smoothness_prior_std: Union[np.array, List[float]]):\n",
    "        \"\"\"\n",
    "        Initialization of a simple point reacher task, where the goal is to reach a point (0, num_links*0,7) using\n",
    "        a robot arm with num_links joints of length 1. Note that this task does *not* use time-series data, but\n",
    "        instead evaluates a single angle configuration.\n",
    "        :param num_links: Number of links of the robot\n",
    "        :param likelihood_std: The \"main\" reward (not regarding the smoothness prior) is the closeness to the point/line.\n",
    "            This reward is represented as a Gaussian with likelihood likelihood_std\n",
    "        :param smoothness_prior_std: Standard deviation of a zero-mean Gaussian acting in angle space.\n",
    "            Adds a smoothness prior for each joint, with smaller values leading to smoother solutions.\n",
    "        \"\"\"\n",
    "        self._num_links = num_links\n",
    "        self.target = [0.7 * num_links, 0]\n",
    "        self._smoothness_likelihood = multivariate_normal(np.zeros(num_links),\n",
    "                                                          np.array(smoothness_prior_std) * np.eye(num_links))\n",
    "        self._target_likelihood = multivariate_normal(self.target, likelihood_std * np.eye(2))\n",
    "\n",
    "    def reward(self, samples: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Calculates the reward for the given angles. Good angle configurations are those that\n",
    "        * reach the target/have a high target (log) likelihood\n",
    "        * are smooth/have a high smoothness (log) likelihood\n",
    "        :param samples: An array of shape (..., num_angles) to evaluate\n",
    "        :return: An array of shape (...), where each entry corresponds to the reward of the\n",
    "          corresponding sample. Higher values are better.\n",
    "        \"\"\"\n",
    "        samples = self.angle_normalize(samples)\n",
    "        end_effector_position = self.forward_kinematic(joint_angles=samples)[..., -1, :]\n",
    "        target_likelihood = self._target_likelihood.logpdf(end_effector_position[..., 0:])\n",
    "        smoothness_likelihood = self._smoothness_likelihood.logpdf(samples)\n",
    "        return np.squeeze(target_likelihood + smoothness_likelihood)\n",
    "\n",
    "    @staticmethod\n",
    "    def forward_kinematic(joint_angles: Union[List[np.array], np.array]) -> Union[List[np.array], np.array]:\n",
    "        \"\"\"\n",
    "        Calculates the forward kinematic of the robot by interpreting each input value as an angle\n",
    "\n",
    "        :param joint_angles: The angles of the joints. Can be of arbitrary shape, as long as the last dimension is over\n",
    "            the relative angles of the robot. I.e., the shape is (..., angles)\n",
    "        :return: The positions as an array of shape (..., #angles, 2),\n",
    "            where the last dimension is the x and y position of each angle.\n",
    "        \"\"\"\n",
    "        angles = np.cumsum(joint_angles, axis=-1)\n",
    "        pos = np.zeros([*angles.shape[:-1], angles.shape[-1] + 1, 2])\n",
    "        for i in range(angles.shape[-1]):\n",
    "            pos[..., i + 1, 0] = pos[..., i, 0] + np.cos(angles[..., i])\n",
    "            pos[..., i + 1, 1] = pos[..., i, 1] + np.sin(angles[..., i])\n",
    "        return pos\n",
    "\n",
    "    @staticmethod\n",
    "    def angle_normalize(angles: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Normalizes the angles to be in [-pi, pi]\n",
    "        :param angles: Unnormalized input angles\n",
    "        :return: Normalized angles\n",
    "        \"\"\"\n",
    "        return ((angles + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "    def render(self, search_distribution: Gaussian, iteration: Union[int, str] = 0):\n",
    "        \"\"\"\n",
    "        Visualize the robot arm by creating an in-place matplotlib figure\n",
    "        :param iteration: The iteration number to use for the figure title\n",
    "        \"\"\"\n",
    "        dimension = search_distribution.task_dimension\n",
    "        plt.gca().add_patch(Rectangle(xy=(-dimension * 0.02, dimension * -0.06),\n",
    "                                      width=dimension * 0.02, height=dimension * 0.12,\n",
    "                                      facecolor=\"grey\", alpha=1, zorder=0))\n",
    "        plt.xlabel(r\"$x$\")\n",
    "        plt.ylabel(r\"$y$\")\n",
    "        axes = plt.gca()\n",
    "        axes.set_xlim([-0.6 * self._num_links, 1.1 * self._num_links])\n",
    "        axes.set_ylim([-0.7 * self._num_links, 0.7 * self._num_links])\n",
    "        axes.set_aspect(aspect=\"equal\")\n",
    "\n",
    "        # plot mean with a very high opacity\n",
    "        mean = search_distribution.mean.squeeze()\n",
    "        mean_angles = self.forward_kinematic(joint_angles=[mean])[0]\n",
    "        mean_reward = self.reward(samples=mean)\n",
    "        plt.plot(mean_angles[:, 0], mean_angles[:, 1], 'go-', markerfacecolor=\"grey\", alpha=0.75,\n",
    "                 label=f\"Mean Reward: {mean_reward:.4e}\")\n",
    "\n",
    "        # plot a high number of samples with low opacity to get a sense of the distribution\n",
    "        samples = search_distribution.sample(100)\n",
    "        angles = self.forward_kinematic(joint_angles=samples)\n",
    "        for position, angle_configuration in enumerate(angles):\n",
    "            plt.plot(angle_configuration[:, 0], angle_configuration[:, 1], 'go-', markerfacecolor=\"grey\", alpha=0.05,\n",
    "                     label=\"Samples\" if position == 0 else None)\n",
    "\n",
    "        plt.scatter(self.target[0], self.target[1], c=\"r\", marker=\"x\", s=100)  # plot target point\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.title(f\"Point Reacher samples and reward at iteration {iteration}\")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Abstract Stochastic Search Class\n",
    "\n",
    "We define an abstract class of the stochastic search algorithms next. The algorithms will inherit from this class and will overwrite algorithm specific parts. You do **not** need to change code in this class, however, you should still make sure that you understand the different functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.189407Z",
     "start_time": "2025-01-23T13:59:12.188053Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.196062Z",
     "start_time": "2025-01-23T13:59:12.190493Z"
    }
   },
   "source": [
    "class AbstractStochasticSearchMethod(abc.ABC):\n",
    "    def __init__(self, task_dimension: int, samples_per_iteration: int, elite_percentage: float = 0.2):\n",
    "        \"\"\"\n",
    "        :param task_dimension: The dimension of the task space\n",
    "        :param samples_per_iteration: The number of samples to draw per iteration\n",
    "        :param elite_percentage: The percentage of samples to keep as elite samples\n",
    "        \"\"\"\n",
    "        smoothness_prior = [1] + [0.04] * (task_dimension - 1)\n",
    "        self._reacher = PointReacher(num_links=task_dimension, likelihood_std=1.0e-4,\n",
    "                                     smoothness_prior_std=smoothness_prior)\n",
    "        self._search_distribution = Gaussian(mean=np.zeros(task_dimension),\n",
    "                                             covariance=np.eye(task_dimension))\n",
    "\n",
    "        self._samples_per_iteration = samples_per_iteration\n",
    "        self._elite_percentage = elite_percentage\n",
    "        self._num_elite_samples = int(self._elite_percentage * self._samples_per_iteration)\n",
    "\n",
    "    def run(self, num_iterations: int, render: bool = True):\n",
    "        full_metrics = {\"mean_reward\": [],\n",
    "                        \"entropy\": []\n",
    "                        }\n",
    "        for iteration in tqdm(range(num_iterations), desc=\"Running Stochastic Search.\"):\n",
    "            # logging utility\n",
    "            if render and (iteration == 0 or 2 ** round(np.log2(iteration)) == iteration):\n",
    "                # plot for each power of 2.\n",
    "                # This gives a lot of plots early on, when the search distribution still changes\n",
    "                # quickly, and eventually slows down near convergence\n",
    "                self.reacher.render(search_distribution=self.search_distribution, iteration=iteration)\n",
    "                save_figure(save_name=f\"method={self.__class__.__name__}_iter={iteration:04d}\")\n",
    "                plot_metrics(full_metrics)\n",
    "                \n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Mean Reward: {self.mean_reward:.4e}, Entropy: {self.entropy:.4e}\")\n",
    "            full_metrics[\"mean_reward\"].append(self.mean_reward)\n",
    "            full_metrics[\"entropy\"].append(self.entropy)\n",
    "\n",
    "            self.step()  # perform one iteration of the search method. This also updates the search distribution\n",
    "\n",
    "        if render:\n",
    "            self.reacher.render(search_distribution=self.search_distribution, iteration=\"final\")\n",
    "            save_figure(save_name=f\"method={self.__class__.__name__}_iter=final\")\n",
    "            plot_metrics(full_metrics)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform one iteration of the cross-entropy method. This includes\n",
    "        1. Sampling from the search distribution\n",
    "        2. Evaluating the samples\n",
    "        3. Updating the search distribution\n",
    "        \"\"\"\n",
    "        samples = self.sample(self._samples_per_iteration)\n",
    "        rewards = self.reacher.reward(samples)\n",
    "\n",
    "        new_mean, new_covariance = self.update_distribution(samples, rewards)\n",
    "        self._search_distribution.update_params(new_mean, new_covariance)\n",
    "\n",
    "    def sample(self, n_samples: int) -> np.array:\n",
    "        \"\"\"\n",
    "        Sample from the search distribution.\n",
    "        :param n_samples: The number of samples to draw\n",
    "        :return: The samples as an array of shape (n_samples, task_dimension)\n",
    "        \"\"\"\n",
    "        return self._search_distribution.sample(n_samples=n_samples)\n",
    "\n",
    "    def update_distribution(self, samples: np.array, rewards: np.array) -> (np.array, np.array):\n",
    "        \"\"\"\n",
    "        Update the search distribution based on the samples and using the cross-entropy method\n",
    "        :param samples: Samples from the search distribution\n",
    "        :param rewards: Rewards of the samples\n",
    "        :return: The new mean and covariance of the search distribution\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def search_distribution(self) -> Gaussian:\n",
    "        return self._search_distribution\n",
    "\n",
    "    @property\n",
    "    def reacher(self) -> PointReacher:\n",
    "        return self._reacher\n",
    "\n",
    "    @property\n",
    "    def mean_reward(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the mean reward of the current search distribution\n",
    "        :return: The mean reward\n",
    "        \"\"\"\n",
    "        mean_reward = self.reacher.reward(self.search_distribution.mean.T)\n",
    "        return mean_reward\n",
    "\n",
    "    @property\n",
    "    def entropy(self) -> float:\n",
    "        return self._search_distribution.entropy"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **TASK 1: Canonical Evolutionary Strategy (CES)** (3 Points)\n",
    "\n",
    "In this task, we will implement the Canonical Evolutionary Strategy (CES). Remember that the algorithms will implement some of the functions defined in the abstract class above and some additional, algorithm specific functions defined in the source code below.\n",
    "The CES follows the classic stochastic search steps defined above. The update of the parameters of the search distribution are done via sample based estimations. This means that the elite samples consisting of the best **M** samples according to the target return are used to estimate the **mean** of our search distribution. Note that CES is a first order method, where the variance is a fixed value and is not optimized.\n",
    "\n",
    "### Task 1.1: The Weight Vector (1 Points)\n",
    "CES makes use of the standard strategy in evolutionary strategies methods in which the drawn samples are ranked. Additionally, when updating the parameters of the search distribution, every sample of the best **M** samples is weighted with a weight $w_i$ which is determined depending on the rank.\n",
    "In this task we will implement this weight vector as shown in **Slide 14** of the **Stochastich Search** slide set.\n",
    "Implement the function `_get_weight_vector()` in the code below. You can access the necessary parameters from the base `AbstractStochasticSearchMethod` class.\n",
    "\n",
    "### Task 1.2: Updating the Parameters in CES (2 Points)\n",
    "Given the weight vector, we can now update the **mean vector** of search distribution.\n",
    "Implement the function `update_distribution(samples: np.array, rewards: np.array)` in the following code cell. Make sure that you correctly estimate the mean, which is a weighted recombination of the samples from the best **M**samples. The update rule can be found on **slide 14** in the slide set **Stochastic Search**. Remember that CES is a first order method and hence, the varinace is fixed ond does not need to be updated.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.201722Z",
     "start_time": "2025-01-23T13:59:12.197105Z"
    }
   },
   "source": [
    "class CanonicalES(AbstractStochasticSearchMethod):\n",
    "    def __init__(self, task_dimension: int, samples_per_iteration: int, elite_percentage: float = 0.01,\n",
    "                 variance: float = 0.00001):\n",
    "        super().__init__(task_dimension, samples_per_iteration, elite_percentage)\n",
    "        \"\"\"\n",
    "        :param task_dimension: The dimension of the task space\n",
    "        :param samples_per_iteration: The number of samples to draw per iteration\n",
    "        :param elite_percentage: The percentage of samples to keep as elite samples\n",
    "        :param variance: Fixed variance for the first order method\n",
    "        \"\"\"\n",
    "        self._weight_vector = self._get_weight_vector()  # precompute weight vector for efficiency\n",
    "        self._variance = variance\n",
    "\n",
    "    def _get_weight_vector(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Create the weight vector for the canonical ES. This is a vector of length samples_per_iteration\n",
    "        that assigns a weight to each sample. The weights are chosen such that the elite samples have\n",
    "        a higher weight than the non-elite samples according to the slides in the lecture.\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        # your code here\n",
    "        weight_vector = np.ones((self._num_elite_samples,1))\n",
    "        weight_vector[:,0] = np.arange(self._num_elite_samples)+1\n",
    "        weight_numerator = np.log(self._num_elite_samples) + 0.5 - np.log(weight_vector)\n",
    "        weight_denumerator = np.sum(weight_numerator, axis=0)\n",
    "        weight_vector = weight_numerator / weight_denumerator\n",
    "        \n",
    "        return weight_vector\n",
    "\n",
    "    def update_distribution(self, samples: np.array, rewards: np.array) -> (np.array, np.array):\n",
    "        \"\"\"\n",
    "        Update the search distribution based on the samples and using the cross-entropy method\n",
    "        :param samples: Samples from the search distribution\n",
    "        :param rewards: Rewards of the samples\n",
    "        :return: The new mean and covariance of the search distribution\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        # your code here\n",
    "        # hint: You will need to use the weight vector here to weight the samples\n",
    "        elite_idxs = rewards.argsort()[-self._num_elite_samples:]\n",
    "        elite_weights = samples[elite_idxs]\n",
    "        new_mean = np.sum(self._weight_vector * elite_weights, axis=0, keepdims=True).T\n",
    "        return new_mean, np.eye(self._search_distribution.mean.shape[0])*self._variance\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **TASK 2: Cross Entropy Method (CEM)** (2 Points)\n",
    "\n",
    "In this task, we will implement the Cross Entropy Method. Remember that the algorithms will implement some of the functions defined in the abstract class above.\n",
    "The Cross Entropy Method follows the classic stochastic search steps defined above. The update of the parameters of the search distribution are done via sample based estimations. This means that the elite samples consisting of the best **M** samples according to the target return are used to estimate the **mean** and **covariance** of our search distribution. Note that CEM is a second order method and hence, it will estimate the **full** covariance of the search distribution.\n",
    "\n",
    "\n",
    "### Task 2.1: Updating the Parameters in CEM (2 Points)\n",
    "Implement the function `update_distribution(samples: np.array, rewards: np.array)` in the following code cell. Make sure that you estimate the **mean and covariance** based on the equations provided on **slide 21** of slide set **Stochastic Search**. Also do not forget that CEM is a second order method and therefore, you need to update the full covariance of the search distribution.\n",
    "\n",
    "Hint: You can make use of the functions `np.mean(x)`, `np.cov(x)`. Do not forget to include the Polyak-Averaging for updating the mean and covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:12.206373Z",
     "start_time": "2025-01-23T13:59:12.202796Z"
    }
   },
   "source": [
    "\n",
    "class CrossEntropyMethod(AbstractStochasticSearchMethod):\n",
    "    def __init__(self, task_dimension: int, samples_per_iteration: int, elite_percentage: float = 0.2):\n",
    "        super().__init__(task_dimension, samples_per_iteration, elite_percentage)\n",
    "        self._alpha = 0.5\n",
    "\n",
    "    def update_distribution(self, samples: np.array, rewards: np.array) -> (np.array, np.array):\n",
    "        \"\"\"\n",
    "        Update the search distribution based on the samples and using the cross-entropy method\n",
    "        :param samples: Samples from the search distribution\n",
    "        :param rewards: Rewards of the samples\n",
    "        :return: The new mean and covariance of the search distribution\n",
    "        \"\"\"\n",
    "        ## TODO ##\n",
    "        # your code here\n",
    "        old_mean = self.search_distribution.mean.squeeze()\n",
    "        old_covariance = self.search_distribution.covariance\n",
    "        elite_idxs = rewards.argsort()[-self._num_elite_samples:]\n",
    "        elite_samples = samples[elite_idxs]\n",
    "        elite_mean = np.array(elite_samples).mean(axis=0)\n",
    "        elite_covariance = np.cov(elite_samples.T,bias=True)\n",
    "\n",
    "        new_mean = (1 - self._alpha) * old_mean + self._alpha * elite_mean\n",
    "        new_covariance = (1 - self._alpha) * old_covariance + self._alpha * elite_covariance\n",
    "\n",
    "        # add a small constant for numerical stability\n",
    "        stable_new_covariance = new_covariance + np.eye(self._search_distribution.task_dimension) * 1e-6\n",
    "        return new_mean, stable_new_covariance\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following cell defines the parameters of the environment (number of robot links), and hyperparameters of the different algorithms.\n",
    "You can choose which algorithm to run via the method argument: Either 'ces' (Canonical Evolutionaory Strategey), or 'cem' (Cross Entropy Method)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T14:03:09.788419Z",
     "start_time": "2025-01-23T14:03:09.783415Z"
    }
   },
   "source": [
    "class Args:\n",
    "\n",
    "    # @markdown Boilerplate for properly accessing the args\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    method = 'ces' # @param {type: \"string\"}\n",
    "    num_links = 15  # @param {type: \"integer\"}\n",
    "    num_iterations = 10000  # @param {type: \"integer\"}\n",
    "    samples_per_iteration = 128  # @param {type: \"integer\"}\n",
    "    ces_variance = 0.0001  # @param {type: \"number\"}"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The next cell will execute the CES and the CEM algortihm.\n",
    "Please submit the **final constellation** of the reacher as well as the **training metrics** for **both** methods with the pre-configured hyperparameters together with your solutions notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T14:03:15.076859Z",
     "start_time": "2025-01-23T14:03:11.092450Z"
    }
   },
   "source": [
    "def main(args: Args):\n",
    "    np.random.seed(0)\n",
    "    num_links = args.num_links\n",
    "    num_iterations = args.num_iterations\n",
    "    method = args.method\n",
    "\n",
    "    if method == \"cem\":\n",
    "        method = CrossEntropyMethod(task_dimension=num_links, samples_per_iteration=args.samples_per_iteration)\n",
    "    elif method == \"ces\":\n",
    "        method = CanonicalES(task_dimension=num_links, samples_per_iteration=args.samples_per_iteration,\n",
    "                             variance=args.ces_variance)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "    # run the search method\n",
    "    method.run(num_iterations=num_iterations)\n",
    "\n",
    "args = Args()\n",
    "main(args=args)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:   0%|          | 2/10000 [00:00<22:49,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -1.0123e+05, Entropy: 2.1284e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:   1%|▏         | 129/10000 [00:01<00:42, 230.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -4.2262e+01, Entropy: -4.7793e+01\n",
      "Mean Reward: -3.0033e+01, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:   5%|▌         | 513/10000 [00:01<00:12, 786.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -2.6984e+01, Entropy: -4.7793e+01\n",
      "Mean Reward: -1.4160e+01, Entropy: -4.7793e+01\n",
      "Mean Reward: -1.0927e+01, Entropy: -4.7793e+01\n",
      "Mean Reward: -6.5548e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -7.3863e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  10%|█         | 1025/10000 [00:01<00:06, 1349.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -1.7316e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.7652e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: -3.4542e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -1.0879e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  16%|█▌        | 1617/10000 [00:01<00:03, 2248.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -7.9099e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: -7.0400e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.0357e-02, Entropy: -4.7793e+01\n",
      "Mean Reward: -2.9128e-02, Entropy: -4.7793e+01\n",
      "Mean Reward: -3.0010e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.4699e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.5710e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.3845e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.4826e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  32%|███▏      | 3232/10000 [00:02<00:01, 3463.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 4.4717e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 8.0141e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: 2.1073e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.9549e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.1238e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.7495e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.1897e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.7368e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.9135e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.0317e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -8.9728e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.9187e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  38%|███▊      | 3826/10000 [00:02<00:01, 4045.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 4.7018e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 2.9644e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 8.3596e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.5298e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 8.1407e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.0687e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.2548e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.2467e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  49%|████▉     | 4889/10000 [00:02<00:01, 3786.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 9.8580e-02, Entropy: -4.7793e+01\n",
      "Mean Reward: -2.3999e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -1.0397e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -2.9998e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.9814e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.9096e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -5.4456e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -2.8311e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: -4.7789e-01, Entropy: -4.7793e+01\n",
      "Mean Reward: -2.4030e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.4605e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.6247e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  61%|██████    | 6075/10000 [00:02<00:00, 4702.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -1.9164e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.3933e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.1582e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.5239e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.7821e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 8.6605e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 7.3773e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.7105e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.2159e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.7728e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.7880e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.8126e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  73%|███████▎  | 7262/10000 [00:03<00:00, 5280.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 4.9433e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.3058e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.5743e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.0139e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.6801e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 2.0297e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.2938e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.7852e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.3175e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.8485e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.8121e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 2.7853e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  78%|███████▊  | 7844/10000 [00:03<00:00, 5430.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 2.1285e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.3349e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 7.3163e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.7327e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.4048e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.:  90%|█████████ | 9007/10000 [00:03<00:00, 4209.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 4.2828e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.6502e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.2306e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.1363e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.1417e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.7138e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.1463e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.9364e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.1916e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.1809e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.4743e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.1387e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Stochastic Search.: 100%|██████████| 10000/10000 [00:03<00:00, 2631.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: 3.7314e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 3.7597e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 1.0709e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 4.6379e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 6.0081e+00, Entropy: -4.7793e+01\n",
      "Mean Reward: 5.4815e+00, Entropy: -4.7793e+01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## MORE\n",
    "### **Task 3: Deriving the MORE equations** (10 Points)\n",
    "Model-based Relative Entropy Stochastic Search (MORE) is a gradient free policy search algorithm for blackbox function optimization.\n",
    "MORE makes use of a Gaussian search distribution with a full covariance matrix, and iteratively updates this distribution by\n",
    "* drawing samples from it\n",
    "* evaluating the samples on the target function\n",
    "* fitting a quadratic surrogate model on the samples and their targets\n",
    "* using this model to update the search distribution in closed form under entropy and KL constraints\n",
    "\n",
    "Comparing this to the general steps outlined above, MORE uses a quadratic surrogate model for a more efficient update of the search distribution.\n",
    "The algorithm is comparatively involved and contains a lot of complex mathematical expressions. We will therefore not implement it, but instead have a look into its theory.\n",
    "\n",
    "More concretely, we are going to derive the primal solution of the optimization problem underlying MORE, given as\n",
    "\\begin{align}\n",
    "    \\underset{\\boldsymbol{\\omega}}{\\textrm{argmax}} \\int p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta})g(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta} \\quad \\textrm{s.t.} \\quad \\textrm{KL}(p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta}) || p_{\\textrm{old}}(\\boldsymbol{\\theta})) \\leq \\epsilon, \\quad \\textrm{H}(p_{\\textrm{old}}(\\boldsymbol{\\theta})) - \\textrm{H}(p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta})) \\leq \\gamma, \\quad \\int p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta}) d \\boldsymbol{\\theta} = 1.\n",
    "\\end{align}\n",
    "\n",
    "As a first simplification we can set $\\beta = \\textrm{H}(p_{\\textrm{old}}(\\boldsymbol{\\theta})) - \\gamma$ and rewrite this objective as\n",
    "\\begin{align}\n",
    "    \\underset{\\boldsymbol{\\omega}}{\\textrm{argmax}} \\int p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta})g(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta} \\quad \\textrm{s.t.} \\quad \\textrm{KL}(p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta}) || p_{\\textrm{old}}(\\boldsymbol{\\theta})) \\leq \\epsilon, \\quad \\textrm{H}(p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta})) \\geq \\beta, \\quad \\int p_{\\boldsymbol{\\omega}}(\\boldsymbol{\\theta}) d \\boldsymbol{\\theta}=1.\n",
    "\\end{align}\n",
    "\n",
    "Denoting the Lagrangian multipliers for the KL and Entropy constraint by $\\eta$ and $\\kappa$ respectivly, **show that $p_{\\boldsymbol{\\omega}^*}(\\boldsymbol{\\theta})$ is the optimal solution to this optimization problem given by**\n",
    "\\begin{align}\n",
    "    p_{\\boldsymbol{\\omega}^*}(\\boldsymbol{\\theta}) \\propto p_{\\textrm{old}}(\\boldsymbol{\\theta})^{\\frac{\\eta}{\\eta + \\kappa}} \\exp\\left( \\dfrac{g(\\boldsymbol{\\theta})}{\\eta + \\kappa} \\right).\n",
    "\\end{align}\n",
    "Note that the optimal solution depends on the duals $\\eta$ and $\\kappa$.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-23T13:59:16.479798Z",
     "start_time": "2025-01-23T13:59:16.478325Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "eQx7oDGeeKWj"
   ],
   "name": "2_dqn_atari.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
